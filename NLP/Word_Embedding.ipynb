{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 목표 : 컴퓨터가 의미를 이해할 수 있도록 자연어를 적절히 수치 표현으로 변환하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**워드 임베딩(Word Embedding)** 이란 단어를 벡터로 표현하는 방법이다.\n",
    "이는 Sparse Representation 에서 Dense Representation 으로 변환하는 것을 의미한다.\n",
    "\n",
    "* **Sparse Representation**\n",
    "        * 벡터 또는 행렬(Matrix)의 값이 대부분이 0으로 표현되는 방법\n",
    "        * one-hot Vectors는 Sparse Vector\n",
    "        * 단어의 갯수가 늘어나면 벡터의 차원이 단어 집합 크기만큼 커짐 --> 공간적 낭비\n",
    "        * 단어의 의미를 담지 못하며 그러므로 단어의 유사성을 표현하지 못함 (두 단어의 내적이 0)\n",
    "\n",
    "## (원핫벡터(차원 커지는거/유사성 표현 못하는거) 예시 들기 : 라이브러리로 one-hot vector 만들기)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어 간 유사성을 고려하기 위해서는 단어의 의미를 벡터화해야 하며, 이러한 표현 방법을 분산 표현(Distributed Representation)이라고 한다.  \n",
    "Distributed Representation은 분포 가설(Distributional Hypothesis)이라는 가정 하에 만들어진 표현 방법이다.  \n",
    "\n",
    "\n",
    "***Distributional Hypothesis***  : 비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다'라는 가정\n",
    "\n",
    "결국 Distributed Representation은 Distributional Hypothesis 가정을 이용하여 코퍼스로부터 단어들의 데이터 셋을 학습하고,  \n",
    "벡터에 단어의 의미를 여러 차원에 분산하여 벡터로 표현하게 된다.\n",
    "\n",
    "<img src=\"./image/1_1.JPG\" width=\"700px\">\n",
    "<img src=\"./image/1_2.JPG\" width=\"700px\">\n",
    "\n",
    "* **Dense Representation**\n",
    "        * 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춤\n",
    "        * 차원이 조밀해지게 되며, 0,1 이외의 실수값도 갖게 됨\n",
    "\n",
    "단어를 Dense Vector의 형태로 표현하는 방법을 Word Embedding이라고 한다.  \n",
    "그리고 Dense Vector를 Embedding 과정을 통해 나온 결과물로 Embedding Vector라고도 한다.  \n",
    "Embedding 방법론으로는 ***LSA, Word2Vec, FastText, Glove 및 케라스에서 제공하는 Embedding() 레이어*** 등이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://wikidocs.net/images/page/22660/word2vec.PNG\" width=\"500px\">\n",
    "\n",
    "    고양이 + 애교 = 강아지\n",
    "    한국 - 서울 + 도쿄 = 일본\n",
    "    박찬호 - 야구 + 축구 = 호나우두\n",
    "\n",
    "위와 같은 연산이 가능한 이유는 각 단어에 있는 상징성 및 의미가 벡터로 계산되었기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec의 학습 방식**은 아래 두 가지로 나뉜다.\n",
    "방법은 다르지만 학습 메커니즘은 거의 동일하다.\n",
    "\n",
    "1. **CBOW(Continuous Bag of Words)**\n",
    "    - 주변에 있는 단어(Context)들을 가지고, 중간에 있는 단어(Target)들을 예측하는 방법\n",
    "    - 아침을 안먹었더니  __ 너무 고프다.\n",
    "1. **Skip-Gram**\n",
    "    - 중간에 있는 단어(Target)로 주변 단어(Context)들들을 예측하는 방법\n",
    "    - __ __  배가 __ __ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자세히 들어가기 전에.... Review Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2차원 평면 위에 반지름이 1인 단위원이 있다고 하면,   \n",
    " 코사인(cosine)의 정의에 의해 cos(θ)는 아래 그림의 녹색 선의 길이와 같다.  \n",
    " (A를 꼭지점으로 하는 직각삼각형의 빗변의 길이는 단위원 반지름인 1)\n",
    "<img src=\"http://i.imgur.com/zCFB0mS.png\" width=\"400px\">\n",
    "\n",
    "    - A가 B에 정확히 포개어져 있을 때 (θ=0도)  --> cos(θ) =  1 (녹색선의 길이가 단위원 반지름과 일치)\n",
    "    - 는 고정한 채 A가 y축 상단으로 이동(θ가 0도에서 90도로 증가)  --> cos(θ)는 0에 수렴\n",
    "\n",
    "<img src=\"http://i.imgur.com/H8WvWMB.gif\" width=\"400px\">\n",
    "\n",
    "cos(θ)는 단위원 내 벡터들끼리의 내적(inner product)과 같다.  \n",
    "**내적값이 커진다는 것은 두 벡터가 이루는 θ가 작아진다(유사도가 높아진다)는 의미로 받아들일 수 있으며, 이를 고차원 벡터공간으로도 확대할 수 있다.**\n",
    "\n",
    "Word2Vec 이러한 코사인과 내적의 성질을 목적함수 구축에 적극 활용하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec은 **은닉층이 하나인 간단한 뉴럴네트워크 구조**이며, 아키텍처는 다음과 같다.\n",
    "\n",
    "* ***CBOW***\n",
    "<img src=\"https://shuuki4.files.wordpress.com/2016/01/cbow.png\" width=\"400px\">\n",
    "\n",
    "* ***Skip-Gram***\n",
    "<img src=\"http://i.imgur.com/TupGxMl.png\" width=\"400px\">\n",
    "\n",
    "\n",
    "위 구조에서 핵심은 **가중치행렬 W, W′** 두 개이다.  \n",
    "**Word2Vec의 학습결과가 이 두 개의 행렬**이다.  \n",
    "입력층-은닉층, 은닉층-출력층을 잇는 **가중치 행렬의 Shape이 서로 전치(transpose)한 것과 동일**하며, 그렇다고 **동일한 행렬은 아니다.**  \n",
    "두 행렬을 하나의 행렬로 취급(tied)하는 방식으로 학습을 진행할 수 있고, 임베딩이 아주 잘되면 W와 W′ 가운데 어떤 걸 단어벡터로 쓰든 관계가 없다고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**V는 임베딩하려는 단어의 수, N은 은닉층의 노드 개수(사용자 지정)이다.** \n",
    "Word2Vec은 **최초 입력으로 one-hot-vector**를 받는다.  \n",
    "1×V 크기의 one-hot-vector의 각 요소와 은닉층의 N개 각 노드는 1대1 대응이 이뤄져야 하므로 **가중치행렬 W의 크기는 V×N**이 된다. \n",
    "\n",
    "<img src=\"http://i.imgur.com/NHUCNXq.png\" width=\"400px\">\n",
    "\n",
    "Word2Vec은 중심단어로 주변단어를 맞추거나, 주변단어로 중심단어를 더 잘 맞추기 위해 **가중치행렬인 W, W′을 조금씩 업데이트하면서 학습을 진행**한다.  \n",
    "W은 one-hot-encoding된 입력벡터와 은닉층을 이어주는 가중치행렬임과 동시에 **Word2Vec의 최종 결과물인 임베딩 단어벡터의 모음**이다.\n",
    "\n",
    "<img src=\"http://i.imgur.com/zuSZWdL.png\" width=\"400px\">\n",
    "Word2Vec의 은닉층을 계산하는 작업은 사실상 가중치행렬 W에서 해당 단어에 해당하는 행벡터를 참조(lookup)해 오는 방식과 똑같다.   \n",
    "\n",
    "학습이 마무리되면 이 W의 행벡터들이 각 단어에 해당하는 임베딩 단어벡터가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-Gram을 학습할 때\n",
    "\n",
    "The quick brown fox jumps over the lazy dog.’ 문장으로 시작하는 학습말뭉치가 있다.  \n",
    "Window(한번에 학습할 단어 개수) 크기가 2인 경우 아래와 같이 슬라이딩 한다.\n",
    "\n",
    "<img src=\"http://i.imgur.com/8zNRwsn.png\" width=\"500px\">\n",
    "\n",
    "중요한 점은 학습할 때 ‘quick’과 ‘brown’을 따로 떼어서 **각각 학습**한다는 점이다.  \n",
    "중심단어에 ‘The’를 넣고 ‘quick’을 주변단어 정답으로 두어서 한번 학습하고, 또 다시 ‘The’를 중심단어로 하고 ‘brown’을 주변단어로 해서 한번 더 학습한다는 것이다.\n",
    "\n",
    "이렇게 첫번째 스텝이 끝나면 중심단어를 오른쪽으로 한칸 옮겨 ‘quick’을 중심단어로 하고, ‘The’, ‘brown’, ‘fox’를 각각 주변단어 정답으로 두는 두번째 스텝을 진행하게 된다.  \n",
    "이런 식으로 **말뭉치 내에 존재하는 모든 단어를 윈도우 크기로 슬라이딩해가며 학습을 하면 iteration 1회가 마무리**된다.\n",
    "\n",
    "CBOW의 경우 중심단어(벡터)는 한번만 업데이트 되지만, 윈도우 크기가 2인 Skip-gram의 경우 중심단어는 4번이나 업데이트 하게 된다. (**4배의 학습량 차이**)  \n",
    "그래서 **Skip-Gram이 CBOW보다 성능이 좋다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 수학적 접근"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec은 아래 식을 최대화하는 걸 목표로 한다.\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "p(o|c)=\\frac { exp({ u }_{ o }^{ T }{ v }_{ c }) }{ \\sum _{ w=1 }^{ W }{ exp({ u }_{ w }^{ T }{ v }_{ c } } )}\n",
    "\\end{equation*}\n",
    "\n",
    " - o : 주변단어(surrounding word)\n",
    " - c :  중심단어(context word)\n",
    " - vc : 중심단어 벡터\n",
    " - uo : 주변단어 벡터\n",
    " - p(o|c): 중심단어(c)가 주어졌을 때 주변단어(o)가 등장할 조건부확률\n",
    "\n",
    "  \n",
    "**결국, 이 식을 최대화하는 것은 중심단어로 주변단어를 잘 맞춘다는 의미이다.**  \n",
    "즉, ‘외나무다리’가 등장했을 때 ‘원수’라는 표현이 나올 것이라는 사실을 잘 예측하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/1.JPG\" width=\"400px\">\n",
    "<img src=\"./image/2.JPG\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그렇다면 위 식 우변을 최대화한다는 말? --> 분자를 키우고, 분모를 줄이면 된다.\n",
    "\n",
    "\\begin{equation*}\n",
    "exp({ u }_{ o }^{ T }{ v }_{ c })\n",
    "\\end{equation*}\n",
    "\n",
    "* 분자를 증가시킨다는 것 :  exp의 지수를 크게 한다는 것 \n",
    "* exp의 지수는 두 벡터의 내적값이 되며, 이 값이 커진다는 건 벡터들 사이의 θ를 줄인다(즉 유사도를 높인다)는 의미이다.\n",
    "* 즉, 중심단어(c)와 주변단어(o)를 벡터공간에 뿌릴 때 인근에 위치시킨다(θ를 줄인다=유사도를 높인다)는 의미로 해석할 수 있다.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum _{ w=1 }^{ W }{ exp({ u }_{ w }^{ T }{ v }_{ c })}\n",
    "\\end{equation*}\n",
    "\n",
    "* 분모는 중심단어(c)와 학습 말뭉치 내 모든 단어를 각각 내적한 것의 총합\n",
    "* 분모를 줄이려면 주변에 등장하지 않은 단어와 중심단어와의 내적값이 작아져야 한다.\n",
    "* 즉, 중심단어 주변에 등장하지 않은 단어(윈도우 크기 내에 등장 X)에 해당하는 벡터와 중심단어 벡터 사이의 θ를 키운다(코사인 유사도를 줄인다)는 의미가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/3.JPG\" width=\"400px\">\n",
    "<img src=\"./image/4.JPG\" width=\"400px\">\n",
    "<img src=\"./image/5.JPG\" width=\"400px\">\n",
    "\n",
    "중심단어 그래디언트의 반대 방향으로 조금씩 중심단어 벡터를 업데이트한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec의 parameter인 W, W′은 크기가 V x N, N x V이며, 단어 수가 늘어날 수록 계산량이 폭증하게 된다.\n",
    "\n",
    "말뭉치에서 **빈번하게 등장하는 단어의 학습량을 확률적인 방식으로 줄이는 방법**이다.  \n",
    "임의의 **threshold보다 더 자주 나오는 단어를 랜덤하게 제거**하게 된다.\n",
    "\n",
    "등장빈도만큼 업데이트 될 기회가 많을 단어들을 제거하며, 학습량을 효과적으로 줄일 수 있다.\n",
    "\n",
    "아래는 i번째 단어(wi)를 학습에서 제외시키기 위한 확률이다.\n",
    "\n",
    "\\begin{equation*}\n",
    "P({ w }_{ i })=1-\\sqrt { \\frac { t }{ f({ w }_{ i }) }  }\n",
    "\\end{equation*}\n",
    "\n",
    "만일 f(wi)가 0.01로 나타나는 빈도 높은 단어(예컨대 조사 ‘은/는’)는 위 식으로 계산한 P(wi)가 0.9684나 되어서 100번의 학습 기회 가운데 96번 정도는 학습에서 제외하게 된다.   \n",
    "반대로 등장 비율이 적어 P(wi)가 0에 가깝다면 해당 단어가 나올 때마다 빼놓지 않고 학습을 시킨다. \n",
    "\n",
    "\n",
    "위 식에서 f(wi)는 해당 단어가 말뭉치에 등장한 비율(해당 단어 빈도/전체 단어수)를 말한다. (자주등장하는 단어일수록 확률값이 줄어들게 됨)  \n",
    "t는 사용자가 지정해주는 빈도가 일정값 이상일 때만 제외하겠다는 느낌의 threshold 값인데,  \n",
    "논문에서는 10^-5 의 값을 사용했을 때 가장 좋은 결과를 얻을 수 있었다고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output을 계산하는 u, v는 window를 얼마로 설정 할 것인지에 따라 그 계산량이 비례하여 증가한다. --> 전체 속도는 떨어진다. (softmax의 summation 증가)   \n",
    "\n",
    "한번 학습할 때, V개의 단어에 대해 softmax를 구하며 weight를 업데이트한다. (하나의 단어의 확률을 맞추기 위해 전체 V가 학습)\n",
    "\n",
    "그래서 몇 개의 단어만 뽑아 Sampling 한다.   \n",
    "전체 단어들에 대해 계산을 하는 대신, 그 중에서 **일부만 뽑아서 softmax 계산을 하고 normalization**을 해주는 것이다.   \n",
    "즉, **계산량은 N x V에서 N x K (K = 뽑는 샘플의 갯수)로 감소**한다.  \n",
    "이 때 실제 target으로 사용하는 단어의 경우 반드시 계산을 해야하므로 이를 ‘positive sample’ 이라고 부르고,  \n",
    "상관없는 단어(Target = 0)를 ‘negative sample’이라고 한다.   \n",
    "negative sample K개를 뽑는 방법을 어떻게 결정하느냐에 따라 Negative sampling의 성능도 달라지고, 이는 보통 실험적으로 결정한다.\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "P(w_i)=\\frac{f(w_i)^{3/4}}{\\sum_{j=0}^n(f(w_j)^{3/4})}\n",
    "\\end{equation*}\n",
    "\n",
    "3/4 는 고정값으로 논문에 따르면 다른 값들에 비해 성능을 가장 잘 내는 값이라고 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분모를 구하는 더하기 작업을 하지 않고도 확률을 구하고자 한다. (기존 softmax의 summation cost를 회피하기 위한 방법)\n",
    "\n",
    "Softmax function의 계산이 오래 걸렸던 것은 확률 계산을 위해 모든 결과에 대한 합을 1로 만들어주기 위함이었다. \n",
    "이 과정에서 최종적으로 나온 output값에 대해 일일히 계산을 해주어서 전체 합으로 normalize를 해주었기 때문에 V 만큼의 계산이 더 필요했던 것이다.\n",
    "\n",
    "Hierarchical Softmax를 사용하면 전체 확률에 대한 계산 없이 전체 합을 1로 만들어 줄 수 있다.\n",
    "\n",
    "각 단어들을 leaves로 가지는 binary tree를 하나 만들어놓은 다음, \n",
    "해당하는 단어의 확률을 계산할 때 **root에서부터 해당 leaf로 가는 path에 따라서 확률을 곱해나가는 식으로 해당 단어가 나올 최종적인 확률을 계산**한다.\n",
    "\n",
    "\n",
    "𝑃(I'm|𝐶)=0.57∗0.68∗0.72=0.28\n",
    "\n",
    "Full binary tree 구조로 변환한 경우 exponential한 스피드 업을 가질 수 있다.\n",
    "예를 들어 1000000개의 문자의 경우, 계산량이 log(1000000)=20 곱셈으로 줄게 된다.\n",
    "\n",
    "특정 노드에서 왼쪽, 오른쪽 자식으로 갈 확률을 더하면 1이 된다. (sigmoid를 사용하기 때문)\n",
    "\n",
    "\n",
    "각 스텝마다 길이 N짜리 벡터 두 개의 내적이 일어나므로 계산량 N이 필요하며,    \n",
    "binary tree를 잘 만들었을 경우 **평균적으로 루트로부터 leaf까지의 거리는 O(lnV)이므로 총 N x lnV의 계산량만으로 특정 단어가 나올 확률을 계산할 수 있다.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hierarchical softmax는 **자주 등장하지 않는 단어에 대해 더 잘 작동**하고,   \n",
    "negative sampling은 **자주 등장하는 단어와 저차원(lower dimensional) 벡터들에 대해 더 잘 작동**하는 경향이 있다.\n",
    "\n",
    "Hierarchical Softmax와 Negative Sampling은 확률 값 계산의 계산량을 줄이기 위한 방법으로 목적이 같다.\n",
    "\n",
    "어떤 문제인지에 따라 Hierarchical Softmax, Negative Sampling 중 어느 한쪽의 성능이 더욱 잘 나올 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
