 ## corpus(말뭉치)를 이용한 unsupervised representation learning
  * pre-training을 통해 얻어진 representation을 직접 활용
    * Word2Vec, ELMO 등
  * pre-trained model을 downstream task에 대해 fine-tuning
    * GPT, BERT 등

 ## causal language model
  * autoregressive model의 또 다른 말
  * 문장에서 단방향 모델을 통해 다음 단어를 예측, 추측하는 언어모델
![141973258-9a4234bd-8daf-4b6d-a339-9283c79d8fb2](https://user-images.githubusercontent.com/61625764/141973258-9a4234bd-8daf-4b6d-a339-9283c79d8fb2.png)

 ## soft one-hot encoding
  * continuous features를 인코딩할 때 사용할 수 있음
  * scalar(수치형 변수)를 피쳐에 대한 임베딩 변환 후, 임베딩의 weighted sum으로 나타내는 방법
  * scalar feature 𝑛, 𝑝<sup>𝑛</sup>=𝑛𝑊<sub>𝑛</sub>+𝑏<sub>𝑛</sub>, where 𝑊<sub>𝑛</sub> ∈ R<sup>1×𝑃</sup> is the weight matrix, 𝑏<sub>𝑛</sub> ∈ R<sup>𝑃</sup> is the bias vector, 𝑃 is the number of desired embeddings for the feature 𝑛 embedding table
  * projection layer(임베딩)에 softmax 취하기, 𝑠<sup>𝑛</sup>=softmax(𝑝<sup>𝑛</sup>)
  * softmax로 얻어진 probability distribution을 weight로 하여 임베딩 공간에 weighted sum 취함
  * 𝑔<sub>𝑛</sub>=𝑠<sup>𝑛</sup>𝐸<sup>𝑛</sup>, where 𝐸<sup>𝑛</sup> ∈ R<sup>𝑃×𝐷</sup> is the embedding matrix for feature 𝑛, and 𝐷 is its embedding size.
  * Li, Yang, Nan Du, and Samy Bengio. "Time-dependent representation for neural event sequence prediction." https://arxiv.org/pdf/1708.00065.pdf (2017).

 ## tying embeddings
  * output layer에 one-hot targets을 쓰지 말고 metric encoded into the space of word embeddings을 쓰자!
  * input embeddings matrix with the output projection layer matrix (input의 임베딩과 output의 임베딩을 같은 걸로 사용 / 다른 blocks에 같은 parameters 사용)
  * 언어 모델(LM)의 경우 모델의 입력과 출력이 모두 단어이므로 동일한 벡터 공간에 있어야 한다는 것
    * 다른 network에 있는 parameters는 서로간의 관계성을 모름)
  * embedding matrices
    * input : the ones you use when feeding context words into a network
    * output : the ones you use before the softmax operation to get predictions
![143230175-1e7c0800-13d5-4b9d-b519-5390d65622c7](https://user-images.githubusercontent.com/61625764/143230175-1e7c0800-13d5-4b9d-b519-5390d65622c7.png)
  * 장점
    * 가까운 단어에 좀 더 높은(유사한/similar) 확률을 제공할 수 있음 (아래의 논문에서 새롭게 제안한 loss function에서의 정규화 항과 유사한 효과가 있음)
    * model size 및 memory 사용을 줄일 수 있음
    * 성능 및 속도 개선(훈련 가능한 변수의 수를 줄이는 것) -> vocab size에 따라 다름
      * rare한 word를 좀 더 잘 표현할 수 있음 (각 training step마다 output layer를 update하는 기존의 방법을 쓸 필요가 없기 때문에)
  * https://lena-voita.github.io/nlp_course/language_modeling.html#paper_weight_tying
  * Inan, Hakan, Khashayar Khosravi, and Richard Socher. "Tying word vectors and word classifiers: A loss framework for language modeling." https://arxiv.org/pdf/1611.01462.pdf (2016).
![143230322-fc37805b-d31e-4cb6-baf4-621db14ae0a7](https://user-images.githubusercontent.com/61625764/143230322-fc37805b-d31e-4cb6-baf4-621db14ae0a7.png)

## 
