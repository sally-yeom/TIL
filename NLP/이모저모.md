1. corpus(ë§ë­‰ì¹˜)ë¥¼ ì´ìš©í•œ unsupervised representation learning
	âƒ pre-trainingì„ í†µí•´ ì–»ì–´ì§„ representationì„ ì§ì ‘ í™œìš©
		âƒ Word2Vec, ELMO ë“±
	âƒ pre-trained modelì„ downstream taskì— ëŒ€í•´ fine-tuning
		âƒ GPT, BERT ë“±
2. causal language model
	- autoregressive modelì˜ ë˜ ë‹¤ë¥¸ ë§
	- ë¬¸ì¥ì—ì„œ ë‹¨ë°©í–¥ ëª¨ë¸ì„ í†µí•´ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡, ì¶”ì¸¡í•˜ëŠ” ì–¸ì–´ëª¨ë¸
<img width="408" alt="causal LM" src="https://user-images.githubusercontent.com/61625764/141973258-9a4234bd-8daf-4b6d-a339-9283c79d8fb2.png">
3. soft one-hot encoding
	- continuous featuresë¥¼ ì¸ì½”ë”©í•  ë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ
	- scalar(ìˆ˜ì¹˜í˜• ë³€ìˆ˜)ë¥¼ í”¼ì³ì— ëŒ€í•œ ì„ë² ë”© ë³€í™˜ í›„, ì„ë² ë”©ì˜ weighted sumìœ¼ë¡œ ë‚˜íƒ€ë‚´ëŠ” ë°©ë²•
	- scalar feature ğ‘›, ğ‘<sup>ğ‘›</sup>=ğ‘›ğ‘Š<sub>ğ‘›</sub>+ğ‘<sub>ğ‘›</sub>, where ğ‘Š<sub>ğ‘›</sub> âˆˆ R<sup>1Ã—ğ‘ƒ</sup> is the weight matrix, ğ‘<sub>ğ‘›</sub> âˆˆ R<sup>ğ‘ƒ</sup> is the bias vector, ğ‘ƒ is the number of desired embeddings for the feature ğ‘› embedding table
	- projection layer(ì„ë² ë”©)ì— softmax ì·¨í•˜ê¸°, ğ‘ <sup>ğ‘›</sup>=softmax(ğ‘<sup>ğ‘›</sup>)
	- softmaxë¡œ ì–»ì–´ì§„ probability distributionì„ weightë¡œ í•˜ì—¬ ì„ë² ë”© ê³µê°„ì— weighted sum ì·¨í•¨
	- ğ‘”<sub>ğ‘›</sub>=ğ‘ <sup>ğ‘›</sup>ğ¸<sup>ğ‘›</sup>, where ğ¸<sup>ğ‘›</sup> âˆˆ R<sup>ğ‘ƒÃ—ğ·</sup> is the embedding matrix for feature ğ‘›, and ğ· is its embedding size.
	- Li, Yang, Nan Du, and Samy Bengio. "Time-dependent representation for neural event sequence prediction." https://arxiv.org/pdf/1708.00065.pdf (2017).
4. tying embeddings
