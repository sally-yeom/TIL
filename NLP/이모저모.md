1. corpus(말뭉치)를 이용한 unsupervised representation learning
	⁃ pre-training을 통해 얻어진 representation을 직접 활용
		⁃ Word2Vec, ELMO 등
	⁃ pre-trained model을 downstream task에 대해 fine-tuning
		⁃ GPT, BERT 등
2. causal language model
	- autoregressive model의 또 다른 말
	- 문장에서 단방향 모델을 통해 다음 단어를 예측, 추측하는 언어모델
<img width="408" alt="causal LM" src="https://user-images.githubusercontent.com/61625764/141973258-9a4234bd-8daf-4b6d-a339-9283c79d8fb2.png">
3. soft one-hot encoding
	- continuous features를 인코딩할 때 사용할 수 있음
	- scalar(수치형 변수)를 피쳐에 대한 임베딩 변환 후, 임베딩의 weighted sum으로 나타내는 방법
	- scalar feature 𝑛, 𝑝<sup>𝑛</sup>=𝑛𝑊<sub>𝑛</sub>+𝑏<sub>𝑛</sub>, where 𝑊<sub>𝑛</sub> ∈ R<sup>1×𝑃</sup> is the weight matrix, 𝑏<sub>𝑛</sub> ∈ R<sup>𝑃</sup> is the bias vector, 𝑃 is the number of desired embeddings for the feature 𝑛 embedding table
	- projection layer(임베딩)에 softmax 취하기, 𝑠<sup>𝑛</sup>=softmax(𝑝<sup>𝑛</sup>)
	- softmax로 얻어진 probability distribution을 weight로 하여 임베딩 공간에 weighted sum 취함
	- 𝑔<sub>𝑛</sub>=𝑠<sup>𝑛</sup>𝐸<sup>𝑛</sup>, where 𝐸<sup>𝑛</sup> ∈ R<sup>𝑃×𝐷</sup> is the embedding matrix for feature 𝑛, and 𝐷 is its embedding size.
	- Li, Yang, Nan Du, and Samy Bengio. "Time-dependent representation for neural event sequence prediction." https://arxiv.org/pdf/1708.00065.pdf (2017).
4. tying embeddings
