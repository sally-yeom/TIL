 ## corpus(ë§ë­‰ì¹˜)ë¥¼ ì´ìš©í•œ unsupervised representation learning
  * pre-trainingì„ í†µí•´ ì–»ì–´ì§„ representationì„ ì§ì ‘ í™œìš©
    * Word2Vec, ELMO ë“±
  * pre-trained modelì„ downstream taskì— ëŒ€í•´ fine-tuning
    * GPT, BERT ë“±

 ## causal language model
  * autoregressive modelì˜ ë˜ ë‹¤ë¥¸ ë§
  * ë¬¸ì¥ì—ì„œ ë‹¨ë°©í–¥ ëª¨ë¸ì„ í†µí•´ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡, ì¶”ì¸¡í•˜ëŠ” ì–¸ì–´ëª¨ë¸
![141973258-9a4234bd-8daf-4b6d-a339-9283c79d8fb2](https://user-images.githubusercontent.com/61625764/141973258-9a4234bd-8daf-4b6d-a339-9283c79d8fb2.png)

 ## soft one-hot encoding
  * continuous featuresë¥¼ ì¸ì½”ë”©í•  ë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ
  * scalar(ìˆ˜ì¹˜í˜• ë³€ìˆ˜)ë¥¼ í”¼ì³ì— ëŒ€í•œ ì„ë² ë”© ë³€í™˜ í›„, ì„ë² ë”©ì˜ weighted sumìœ¼ë¡œ ë‚˜íƒ€ë‚´ëŠ” ë°©ë²•
  * scalar feature ğ‘›, ğ‘<sup>ğ‘›</sup>=ğ‘›ğ‘Š<sub>ğ‘›</sub>+ğ‘<sub>ğ‘›</sub>, where ğ‘Š<sub>ğ‘›</sub> âˆˆ R<sup>1Ã—ğ‘ƒ</sup> is the weight matrix, ğ‘<sub>ğ‘›</sub> âˆˆ R<sup>ğ‘ƒ</sup> is the bias vector, ğ‘ƒ is the number of desired embeddings for the feature ğ‘› embedding table
  * projection layer(ì„ë² ë”©)ì— softmax ì·¨í•˜ê¸°, ğ‘ <sup>ğ‘›</sup>=softmax(ğ‘<sup>ğ‘›</sup>)
  * softmaxë¡œ ì–»ì–´ì§„ probability distributionì„ weightë¡œ í•˜ì—¬ ì„ë² ë”© ê³µê°„ì— weighted sum ì·¨í•¨
  * ğ‘”<sub>ğ‘›</sub>=ğ‘ <sup>ğ‘›</sup>ğ¸<sup>ğ‘›</sup>, where ğ¸<sup>ğ‘›</sup> âˆˆ R<sup>ğ‘ƒÃ—ğ·</sup> is the embedding matrix for feature ğ‘›, and ğ· is its embedding size.
  * Li, Yang, Nan Du, and Samy Bengio. "Time-dependent representation for neural event sequence prediction." https://arxiv.org/pdf/1708.00065.pdf (2017).

 ## tying embeddings
  * output layerì— one-hot targetsì„ ì“°ì§€ ë§ê³  metric encoded into the space of word embeddingsì„ ì“°ì!
  * input embeddings matrix with the output projection layer matrix (inputì˜ ì„ë² ë”©ê³¼ outputì˜ ì„ë² ë”©ì„ ê°™ì€ ê±¸ë¡œ ì‚¬ìš© / ë‹¤ë¥¸ blocksì— ê°™ì€ parameters ì‚¬ìš©)
  * ì–¸ì–´ ëª¨ë¸(LM)ì˜ ê²½ìš° ëª¨ë¸ì˜ ì…ë ¥ê³¼ ì¶œë ¥ì´ ëª¨ë‘ ë‹¨ì–´ì´ë¯€ë¡œ ë™ì¼í•œ ë²¡í„° ê³µê°„ì— ìˆì–´ì•¼ í•œë‹¤ëŠ” ê²ƒ
    * ë‹¤ë¥¸ networkì— ìˆëŠ” parametersëŠ” ì„œë¡œê°„ì˜ ê´€ê³„ì„±ì„ ëª¨ë¦„)
  * embedding matrices
    * input : the ones you use when feeding context words into a network
    * output : the ones you use before the softmax operation to get predictions
![143230175-1e7c0800-13d5-4b9d-b519-5390d65622c7](https://user-images.githubusercontent.com/61625764/143230175-1e7c0800-13d5-4b9d-b519-5390d65622c7.png)
  * ì¥ì 
    * ê°€ê¹Œìš´ ë‹¨ì–´ì— ì¢€ ë” ë†’ì€(ìœ ì‚¬í•œ/similar) í™•ë¥ ì„ ì œê³µí•  ìˆ˜ ìˆìŒ (ì•„ë˜ì˜ ë…¼ë¬¸ì—ì„œ ìƒˆë¡­ê²Œ ì œì•ˆí•œ loss functionì—ì„œì˜ ì •ê·œí™” í•­ê³¼ ìœ ì‚¬í•œ íš¨ê³¼ê°€ ìˆìŒ)
    * model size ë° memory ì‚¬ìš©ì„ ì¤„ì¼ ìˆ˜ ìˆìŒ
    * ì„±ëŠ¥ ë° ì†ë„ ê°œì„ (í›ˆë ¨ ê°€ëŠ¥í•œ ë³€ìˆ˜ì˜ ìˆ˜ë¥¼ ì¤„ì´ëŠ” ê²ƒ) -> vocab sizeì— ë”°ë¼ ë‹¤ë¦„
      * rareí•œ wordë¥¼ ì¢€ ë” ì˜ í‘œí˜„í•  ìˆ˜ ìˆìŒ (ê° training stepë§ˆë‹¤ output layerë¥¼ updateí•˜ëŠ” ê¸°ì¡´ì˜ ë°©ë²•ì„ ì“¸ í•„ìš”ê°€ ì—†ê¸° ë•Œë¬¸ì—)
  * https://lena-voita.github.io/nlp_course/language_modeling.html#paper_weight_tying
  * Inan, Hakan, Khashayar Khosravi, and Richard Socher. "Tying word vectors and word classifiers: A loss framework for language modeling." https://arxiv.org/pdf/1611.01462.pdf (2016).
![143230322-fc37805b-d31e-4cb6-baf4-621db14ae0a7](https://user-images.githubusercontent.com/61625764/143230322-fc37805b-d31e-4cb6-baf4-621db14ae0a7.png)

## 
